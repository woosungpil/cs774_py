Theano implementation of dropout.  See: http://arxiv.org/abs/1207.0580

Run with:

     ./mlp.py dropout

for dropout, or 

    ./mlp.py backprop

for regular backprop with no dropout.

**** bit modified (12/01)
    ./mlp.py [dropout|backprop] [unsup_pretrain] [ReLU | Tanh | Sigmoid | Softplus] [0.4,0.6]
    * rate should be continued with comma
    ** Don't care about order of parameters
**** Jinpyo
Use:

    ./plot_results.sh results.png

to visualize the results.

Based on code from:
- http://deeplearning.net/tutorial/mlp.html
- http://deeplearning.net/tutorial/logreg.html

Use the data here to make the units of the results comparable to Hinton's paper:
- http://www.cs.ox.ac.uk/people/misha.denil/hidden/mnist_batches.npz

ver 1
